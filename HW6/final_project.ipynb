{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "80\n",
      "80\n",
      "80\n",
      "80\n",
      "100000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import pickle\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering , RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer , BertForSequenceClassification\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def process_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ').replace('\\t' , ' ')\n",
    "    text = text.strip()\n",
    "    return text.split()\n",
    "\n",
    "def get_rel_score(query,document):\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "def get_chunks(document,window_size,stride):\n",
    "    chunks = []\n",
    "    for i in range(0,len(document),stride):\n",
    "        chunks.append(document[i : i+window_size])\n",
    "    return chunks\n",
    "\n",
    "# def load_data():\n",
    "doc_df = pd.read_csv('documents.csv')\n",
    "docs_id = doc_df['doc_id'].tolist()\n",
    "docs = doc_df['doc_text'].apply(process_text).tolist()\n",
    "doc_id2idx = {}\n",
    "for i in range(len(docs_id)):\n",
    "    doc_id2idx[docs_id[i]] = i\n",
    "train_query_df = pd.read_csv('train_queries.csv')\n",
    "train_queries = train_query_df['query_text'].apply(process_text).tolist()\n",
    "train_queries_id = train_query_df['query_id'].astype('str').tolist()\n",
    "train_queries_pos_docs = train_query_df['pos_doc_ids'].tolist()\n",
    "train_queries_top_bm25 = train_query_df['bm25_top1000'].tolist()\n",
    "train_queries_top_bm25_scores = train_query_df['bm25_top1000_scores'].tolist()\n",
    "\n",
    "print(len(train_queries))\n",
    "print(len(train_queries_id))\n",
    "print(len(train_queries_pos_docs))\n",
    "print(len(train_queries_top_bm25))\n",
    "print(len(train_queries_top_bm25_scores))\n",
    "test_query_df = pd.read_csv('test_queries.csv')\n",
    "test_queries = test_query_df['query_text'].apply(process_text).tolist()\n",
    "test_queries_id = test_query_df['query_id'].astype('str').tolist()\n",
    "test_queries_top_bm25 = test_query_df['bm25_top1000'].tolist()\n",
    "test_queries_top_bm25_scores = test_query_df['bm25_top1000_scores'].tolist()\n",
    "\n",
    "with open('doc_chunks', 'rb') as handle:\n",
    "    doc_chunks = pickle.load(handle)\n",
    "\n",
    "print(len(test_queries))\n",
    "print(len(test_queries_id))\n",
    "print(len(test_queries_top_bm25))\n",
    "print(len(test_queries_top_bm25_scores))\n",
    "print(len(doc_chunks))\n",
    "\n",
    "pretrained_model = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(pretrained_model)\n",
    "device='cuda'\n",
    "checkpoint = 'bert-base-uncased_epoch1_size120000'\n",
    "model.load_state_dict(torch.load(checkpoint,map_location='cpu'))\n",
    "model.eval().to(device)\n",
    "\n",
    "class TestDataset(Dataset):\n",
    "    def __init__(self, input_ids, token_type_ids , attention_mask):\n",
    "        self.input_ids = input_ids\n",
    "        self.token_type_ids = token_type_ids\n",
    "        self.attention_mask = attention_mask        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        return inputid , tokentype , attentionmask\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relqc(query,chunks):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    query = ' '.join(query)\n",
    "    for chunk in chunks:\n",
    "        chunk = ' '.join(chunk)\n",
    "        tmp_dict =  tokenizer(query,\n",
    "                          chunk,\n",
    "                          max_length=20,\n",
    "                          return_tensors='pt',\n",
    "                          return_token_type_ids = True,\n",
    "                          pad_to_max_length=True,\n",
    "                          padding='max_length',\n",
    "                          truncation=True)\n",
    "        input_ids.append(tmp_dict['input_ids'][0])\n",
    "        token_type_ids.append(tmp_dict['token_type_ids'][0])\n",
    "        attention_mask.append(tmp_dict['attention_mask'][0])\n",
    "    BATCH_SIZE = 32\n",
    "    test_set = TestDataset(input_ids, token_type_ids,attention_mask)\n",
    "    loader =  DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=False)\n",
    "    model_scores = np.array([])\n",
    "    for data in loader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                token_type_ids=segments_tensors, \n",
    "                attention_mask=masks_tensors)\n",
    "        batch_scores = outputs[0][:,1].detach().cpu().numpy()\n",
    "        model_scores = np.append(model_scores,batch_scores)\n",
    "\n",
    "    # topk_indices = model_scores.argsort()[-topk:][::-1]\n",
    "    # print(topk_indices)\n",
    "    return model_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relcd(chunks,document):\n",
    "    input_ids = []\n",
    "    token_type_ids = []\n",
    "    attention_mask = []\n",
    "    document = ' '.join(document)\n",
    "    for chunk in chunks:\n",
    "        query = ' '.join(chunk)\n",
    "        tmp_dict =  tokenizer(query,\n",
    "                          document,\n",
    "                          max_length=512,\n",
    "                          return_tensors='pt',\n",
    "                          return_token_type_ids = True,\n",
    "                          pad_to_max_length=True,\n",
    "                          padding='max_length',\n",
    "                          truncation=True)\n",
    "        input_ids.append(tmp_dict['input_ids'][0])\n",
    "        token_type_ids.append(tmp_dict['token_type_ids'][0])\n",
    "        attention_mask.append(tmp_dict['attention_mask'][0])\n",
    "    BATCH_SIZE = 16\n",
    "    test_set = TestDataset(input_ids, token_type_ids,attention_mask)\n",
    "    loader =  DataLoader(test_set, batch_size=BATCH_SIZE,shuffle=False)\n",
    "#     print(len(loader))\n",
    "    model_scores = np.array([])\n",
    "    for data in loader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors = [t.to(device) for t in data]\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                token_type_ids=segments_tensors, \n",
    "                attention_mask=masks_tensors)\n",
    "        batch_scores = outputs[0][:,1].detach().cpu().numpy()\n",
    "        model_scores = np.append(model_scores,batch_scores)\n",
    "\n",
    "    # topk_indices = model_scores.argsort()[-topk:][::-1]\n",
    "    # print(topk_indices)\n",
    "    return model_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('doc_chunk_dict', 'rb') as handle:\n",
    "    doc_chunk_dict = pickle.load(handle)\n",
    "with open('rel_query_topkdoc_chunks', 'rb') as handle:\n",
    "    rel_query_topkdoc_chunks = pickle.load(handle)\n",
    "with open('rel_qds', 'rb') as handle:\n",
    "    rel_qds = pickle.load(handle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n",
    "    e_x = np.exp(x - np.max(x))\n",
    "    return e_x / e_x.sum(axis=0) # only difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('query_rel_cd_scores', 'rb') as handle:\n",
    "    query_rel_cd_scores = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 301 ['international', 'organized', 'crime']\n",
      "1 303 ['hubble', 'telescope', 'achievements']\n",
      "2 304 ['endangered', 'species', 'mammals']\n",
      "3 306 ['african', 'civilian', 'deaths']\n",
      "4 311 ['industrial', 'espionage']\n",
      "5 313 ['magnetic', 'levitationmaglev']\n",
      "6 314 ['marine', 'vegetation']\n",
      "7 315 ['unexplained', 'highway', 'accidents']\n",
      "8 316 ['polygamy', 'polyandry', 'polygyny']\n",
      "9 317 ['unsolicited', 'faxes']\n",
      "10 320 ['undersea', 'fiber', 'optic', 'cable']\n",
      "11 331 ['world', 'bank', 'criticism']\n",
      "12 333 ['antibiotics', 'bacteria', 'disease']\n",
      "13 337 ['viral', 'hepatitis']\n",
      "14 338 ['risk', 'of', 'aspirin']\n",
      "15 339 ['alzheimers', 'drug', 'treatment']\n",
      "16 343 ['police', 'deaths']\n",
      "17 344 ['abuses', 'of', 'email']\n",
      "18 345 ['overseas', 'tobacco', 'sales']\n",
      "19 346 ['educational', 'standards']\n",
      "20 347 ['wildlife', 'extinction']\n",
      "21 348 ['agoraphobia']\n",
      "22 353 ['antarctica', 'exploration']\n",
      "23 355 ['ocean', 'remote', 'sensing']\n",
      "24 361 ['clothing', 'sweatshops']\n",
      "25 362 ['human', 'smuggling']\n",
      "26 365 ['el', 'nino']\n",
      "27 366 ['commercial', 'cyanide', 'uses']\n",
      "28 367 ['piracy']\n",
      "29 370 ['fooddrug', 'laws']\n",
      "30 374 ['nobel', 'prize', 'winners']\n",
      "31 377 ['cigar', 'smoking']\n",
      "32 379 ['mainstreaming']\n",
      "33 380 ['obesity', 'medical', 'treatment']\n",
      "34 381 ['alternative', 'medicine']\n",
      "35 385 ['hybrid', 'fuel', 'cars']\n",
      "36 396 ['sick', 'building', 'syndrome']\n",
      "37 402 ['behavioral', 'genetics']\n",
      "38 403 ['osteoporosis']\n",
      "39 404 ['ireland', 'peace', 'talks']\n",
      "40 405 ['cosmic', 'events']\n",
      "41 406 ['parkinsons', 'disease']\n",
      "42 407 ['poaching', 'wildlife', 'preserves']\n",
      "43 411 ['salvaging', 'shipwreck', 'treasure']\n",
      "44 412 ['airport', 'security']\n",
      "45 414 ['cuba', 'sugar', 'exports']\n",
      "46 416 ['three', 'gorges', 'project']\n",
      "47 422 ['art', 'stolen', 'forged']\n",
      "48 423 ['milosevic', 'mirjana', 'markovic']\n",
      "49 426 ['law', 'enforcement', 'dogs']\n",
      "50 427 ['uv', 'damage', 'eyes']\n",
      "51 429 ['legionnaires', 'disease']\n",
      "52 431 ['robotic', 'technology']\n",
      "53 436 ['railway', 'accidents']\n",
      "54 437 ['deregulation', 'gas', 'electric']\n",
      "55 445 ['women', 'clergy']\n",
      "56 446 ['tourists', 'violence']\n",
      "57 447 ['stirling', 'engine']\n",
      "58 604 ['lyme', 'disease', 'arthritis']\n",
      "59 607 ['human', 'genetic', 'code']\n",
      "60 609 ['per', 'capita', 'alcohol', 'consumption']\n",
      "61 610 ['minimum', 'wage', 'adverse', 'impact']\n",
      "62 611 ['kurds', 'germany', 'violence']\n",
      "63 612 ['tibet', 'protesters']\n",
      "64 613 ['berlin', 'wall', 'disposal']\n",
      "65 616 ['volkswagen', 'mexico']\n",
      "66 618 ['ayatollah', 'khomeini', 'death']\n",
      "67 621 ['women', 'ordained', 'church', 'of', 'england']\n",
      "68 622 ['price', 'fixing']\n",
      "69 623 ['toxic', 'chemical', 'weapon']\n",
      "70 627 ['russian', 'food', 'crisis']\n",
      "71 628 ['us', 'invasion', 'of', 'panama']\n",
      "72 630 ['gulf', 'war', 'syndrome']\n",
      "73 637 ['human', 'growth', 'hormone', 'hgh']\n",
      "74 638 ['wrongful', 'convictions']\n",
      "75 643 ['salmon', 'dams', 'pacific', 'northwest']\n",
      "76 644 ['exotic', 'animals', 'import']\n",
      "77 645 ['software', 'piracy']\n",
      "78 646 ['food', 'stamps', 'increase']\n",
      "79 647 ['windmill', 'electricity']\n"
     ]
    }
   ],
   "source": [
    "topkd = 10\n",
    "topkc = 10\n",
    "rel_query_topkdoc_chunks = []\n",
    "phase3_scores = []\n",
    "query_rel_cd_scores = []\n",
    "\n",
    "# phase2_scores = []\n",
    "with open('phase2_scores', 'rb') as handle:\n",
    "    phase2_scores = pickle.load(handle)\n",
    "    \n",
    "with open('query_rel_cd_scores', 'rb') as handle:\n",
    "    query_rel_cd_scores = pickle.load(handle)\n",
    "\n",
    "# with open('phase2_scores', 'rb') as handle:\n",
    "#     phase2_scores = pickle.load(handle)\n",
    "for i in range(len(test_queries)):\n",
    "    query = test_queries[i]\n",
    "    query_id = test_queries_id[i]\n",
    "    print(i,query_id , query)\n",
    "    rel_qd = rel_qds[i]\n",
    "    bm25_top1000_document_ids = test_queries_top_bm25[i].split()\n",
    "\n",
    "\n",
    "#    phase 1 output\n",
    "    topk_first_rerank_docs_indices = rel_qd.argsort()[-topkd:][::-1]\n",
    "    \n",
    "    \n",
    "#   phase 2 \n",
    "    \n",
    "    be_selected_chunks = []\n",
    "    \n",
    "    for rerank_docs_index in topk_first_rerank_docs_indices:\n",
    "        doc_id = bm25_top1000_document_ids[rerank_docs_index]\n",
    "        doc_idx = doc_id2idx[doc_id]\n",
    "        chunks = doc_chunks[doc_idx]\n",
    "        be_selected_chunks.extend(chunks)\n",
    "    \n",
    "#     phase2_score = get_relqc(query,be_selected_chunks)\n",
    "#     phase2_scores.append(phase2_score)\n",
    "    \n",
    "    phase2_score = phase2_scores[i]\n",
    "    topk_chunks = []\n",
    "    rel_qc_score = []\n",
    "    \n",
    "    topk_chunks_indices = phase2_score.argsort()[-topkc:][::-1]\n",
    "    for index in topk_chunks_indices:\n",
    "        topk_chunks.append(be_selected_chunks[index])\n",
    "        rel_qc_score.append(phase2_score[index])\n",
    "    \n",
    "    softmax_rel_qc_score = softmax(rel_qc_score)\n",
    "    softmax_rel_qc_score = np.array(softmax_rel_qc_score)\n",
    "\n",
    "#     phase 3\n",
    "    rel_qcds = []\n",
    "    rel_cd_scores = query_rel_cd_scores[i]\n",
    "\n",
    "    #     rel_cd_scores = []\n",
    "    index = 0\n",
    "    for doc_id in bm25_top1000_document_ids:\n",
    "        doc_idx = doc_id2idx[doc_id]\n",
    "        doc = docs[doc_idx]\n",
    "        \n",
    "#         rel_cd_score = get_relcd(topk_chunks,doc)\n",
    "#         rel_cd_scores.append(rel_cd_score)\n",
    "        rel_cd_score = rel_cd_scores[index]\n",
    "        index += 1\n",
    "        rel_cd_score = np.array(rel_cd_score)\n",
    "#         print(rel_cd_score.shape,softmax_rel_qc_score.shape)\n",
    "        final_score = np.dot(rel_cd_score,softmax_rel_qc_score)\n",
    "        \n",
    "        rel_qcds.append(final_score)\n",
    "    phase3_scores.append(rel_qcds)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# with open('query_rel_cd_scores', 'wb') as handle:\n",
    "#     pickle.dump(query_rel_cd_scores, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "        \n",
    "#     phase3_scores.append(rel_qcds)\n",
    "\n",
    "# with open('phase2_scores', 'wb') as handle:\n",
    "#     pickle.dump(phase2_scores, handle, protocol=pickle.HIGHEST_PROTOCOL)      \n",
    "\n",
    "        \n",
    "# #     phase 3\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# # with open('doc_chunk_dict', 'wb') as handle:\n",
    "# #     pickle.dump(doc_chunk_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    "\n",
    "# with open('phase3_scores', 'wb') as handle:\n",
    "#     pickle.dump(phase3_scores, handle, protocol=pickle.HIGHEST_PROTOCOL) \n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "with open('rel_qds', 'rb') as handle:\n",
    "    rel_qds = pickle.load(handle)\n",
    "\n",
    "# with open('phase3_scores', 'rb') as handle:\n",
    "#     phase3_scores = pickle.load(handle)\n",
    "    \n",
    "print(len(rel_qds))\n",
    "print(len(phase3_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minmax_normal(score):\n",
    "    return (score - score.min()) / (score.max() - score.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./final/BERT_QE_0.0.txt\n",
      "./final/BERT_QE_0.1.txt\n",
      "./final/BERT_QE_0.2.txt\n",
      "./final/BERT_QE_0.30000000000000004.txt\n",
      "./final/BERT_QE_0.4.txt\n",
      "./final/BERT_QE_0.5.txt\n",
      "./final/BERT_QE_0.6000000000000001.txt\n",
      "./final/BERT_QE_0.7000000000000001.txt\n",
      "./final/BERT_QE_0.8.txt\n",
      "./final/BERT_QE_0.9.txt\n",
      "./final/BERT_QE_1.0.txt\n"
     ]
    }
   ],
   "source": [
    "for a in np.arange(0.0,1.1,0.1):\n",
    "\n",
    "    filename = './final/BERT_QE_{}.txt'.format(a)\n",
    "    print(filename)\n",
    "    fp = open(filename, 'w')\n",
    "    print('query_id,ranked_doc_ids', file=fp)\n",
    "\n",
    "    for i in range(len(test_queries)):\n",
    "        query = test_queries[i]\n",
    "        query_id = test_queries_id[i]\n",
    "\n",
    "        \n",
    "        print(query_id + ',', file=fp, end='')\n",
    "        rel_qd = rel_qds[i]\n",
    "        phase3_score = phase3_scores[i]\n",
    "        query_bm25_doc_id = test_queries_top_bm25[i].split()\n",
    "        rel_qd = np.array(rel_qd)\n",
    "        phase3_score = np.array(phase3_score)\n",
    "\n",
    "        final_scores = (1-a) * rel_qd + a * phase3_score\n",
    "        \n",
    "        doc_dict = dict(zip(query_bm25_doc_id, final_scores))\n",
    "        sorted_docs = sorted(doc_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#         print(sorted_docs[:20])\n",
    "\n",
    "        for _doc in sorted_docs:\n",
    "            doc_id, value = _doc\n",
    "            print(doc_id, file=fp, end=' ')\n",
    "        print('',file=fp)\n",
    "\n",
    "\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./final/BERT_QE0.0_BM252.3.txt\n",
      "./final/BERT_QE0.1_BM252.3.txt\n",
      "./final/BERT_QE0.2_BM252.3.txt\n",
      "./final/BERT_QE0.30000000000000004_BM252.3.txt\n",
      "./final/BERT_QE0.4_BM252.3.txt\n",
      "./final/BERT_QE0.5_BM252.3.txt\n",
      "./final/BERT_QE0.6000000000000001_BM252.3.txt\n",
      "./final/BERT_QE0.7000000000000001_BM252.3.txt\n",
      "./final/BERT_QE0.8_BM252.3.txt\n",
      "./final/BERT_QE0.9_BM252.3.txt\n",
      "./final/BERT_QE1.0_BM252.3.txt\n"
     ]
    }
   ],
   "source": [
    "bm25_alpha = 2.3\n",
    "for a in np.arange(0.0,1.1,0.1):\n",
    "    \n",
    "    \n",
    "\n",
    "    filename = './final/BERT_QE{}_BM25{}.txt'.format(a,bm25_alpha)\n",
    "    print(filename)\n",
    "    fp = open(filename, 'w')\n",
    "    print('query_id,ranked_doc_ids', file=fp)\n",
    "\n",
    "    for i in range(len(test_queries)):\n",
    "        query = test_queries[i]\n",
    "        query_id = test_queries_id[i]\n",
    "        tmp_bm25_score = test_queries_top_bm25_scores[i].split()\n",
    "    \n",
    "\n",
    "        bm25_score = []\n",
    "        for score in tmp_bm25_score:\n",
    "            bm25_score.append(float(score))\n",
    "        bm25_score = np.array(bm25_score)\n",
    "        bm25_score = minmax_normal(bm25_score)\n",
    "\n",
    "        if query_id == '316':\n",
    "            bm25_score = np.array([0.0]*1000)\n",
    "        \n",
    "        \n",
    "        print(query_id + ',', file=fp, end='')\n",
    "        rel_qd = rel_qds[i]\n",
    "        phase3_score = phase3_scores[i]\n",
    "        query_bm25_doc_id = test_queries_top_bm25[i].split()\n",
    "        rel_qd = np.array(rel_qd)\n",
    "        phase3_score = np.array(phase3_score)\n",
    "\n",
    "        final_scores = (1-a) * rel_qd + a * phase3_score\n",
    "        final_scores = minmax_normal(final_scores)\n",
    "        \n",
    "        final_scores = bm25_score + bm25_alpha * final_scores\n",
    "        \n",
    "        doc_dict = dict(zip(query_bm25_doc_id, final_scores))\n",
    "        sorted_docs = sorted(doc_dict.items(), key=lambda x: x[1], reverse=True)\n",
    "#         print(sorted_docs[:20])\n",
    "\n",
    "        for _doc in sorted_docs:\n",
    "            doc_id, value = _doc\n",
    "            print(doc_id, file=fp, end=' ')\n",
    "        print('',file=fp)\n",
    "\n",
    "\n",
    "    fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['international', 'organized', 'crime']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_queries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['language', 'f', 'p105', 'chinese', 'f', 'article', 'typebfn', 'f', 'p106', 'special', 'article', 'by', 'staff', 'reporter', 'chang', 'shaowei', '1728', 'f', '1421', '1218', 'china', 'and', 'the', 'united', 'states', 'increase', 'cooperation', 'in', 'cracking', 'down', 'on', 'crimes', 'text', 'the', 'chinese', 'and', 'us', 'governments', 'have', 'clear', 'differences', 'in', 'terms', 'of', 'human', 'rights', 'missile', 'proliferation', 'and', 'trade', 'methods', 'and', 'we', 'neither', 'deny', 'nor', 'ignore', 'them', 'however', 'since', 'internationalized', 'crime', 'poses', 'a', 'common', 'threat', 'to', 'both', 'our', 'countries', 'we', 'are', 'making', 'joint', 'efforts', 'to', 'face', 'squarely', 'the', 'common', 'interests', 'we', 'have', 'on', 'this', 'issue', 'that', 'is', 'the', 'view', 'aired', 'by', 'robert', 'gelbard', 'us', 'assistant', 'secretary', 'of', 'state', 'for', 'international', 'narcotics', 'matters', 'in', 'a', 'speech', 'in', 'hong', 'kong', 'yesterday', 'it', 'was', 'precisely', 'for', 'this', 'reason', 'that', 'gelbard', 'recently', 'led', 'a', 'highlevel', 'delegation', 'to', 'china', 'to', 'seek', 'increased', 'cooperation', 'in', 'cracking', 'down', 'on', 'crime', 'since', 'president', 'jiang', 'zemin', 'and', 'president', 'clinton', 'met', 'last', 'november', 'sinous', 'relations', 'have', 'further', 'improved', 'gelbards', 'trip', 'proves', 'that', 'as', 'two', 'big', 'powers', 'china', 'and', 'the', 'united', 'states', 'have', 'many', 'spheres', 'of', 'cooperation', 'with', 'the', 'rise', 'in', 'international', 'organized', 'crime', 'the', 'focus', 'of', 'the', 'crackdown', 'has', 'shifted', 'to', 'how', 'to', 'get', 'more', 'information', 'to', 'accurately', 'deal', 'a', 'crushing', 'blow', 'from', 'top', 'to', 'bottom', 'at', 'transnational', 'crime', 'syndicates', 'this', 'makes', 'it', 'necessary', 'for', 'the', 'lawenforcement', 'departments', 'of', 'various', 'countries', 'to', 'work', 'with', 'good', 'cooperation', 'in', 'his', 'speech', 'gelbard', 'described', 'how', 'transnational', 'organized', 'crime', 'syndicates', 'carry', 'out', 'illegal', 'activities', 'with', 'advanced', 'weapons', 'telecommunications', 'and', 'transport', 'facilities', 'as', 'well', 'as', 'monetary', 'means', 'he', 'said', 'if', 'the', 'boss', 'of', 'an', 'asian', 'syndicate', 'enterprise', 'wants', 'to', 'fix', 'up', 'a', 'kidnapping', 'case', 'in', 'new', 'york', 'he', 'can', 'order', 'his', 'subordinate', 'in', 'new', 'jersey', 'by', 'directly', 'dialing', 'his', 'subordinates', 'mobile', 'telephone', 'number', 'using', 'satellite', 'telecommunications', 'equipment', 'afterward', 'they', 'can', 'make', 'contact', 'again', 'by', 'coded', 'fax', 'if', 'funds', 'are', 'needed', 'the', 'money', 'can', 'be', 'remitted', 'to', 'the', 'united', 'states', 'electronically', 'possibly', 'through', 'a', 'hong', 'kong', 'bank', 'the', 'ransom', 'can', 'also', 'be', 'remitted', 'to', 'a', 'bank', 'in', 'the', 'bahamas', 'then', 'sent', 'again', 'from', 'there', 'to', 'panama', 'eventually', 'arriving', 'in', 'a', 'numbered', 'swiss', 'bank', 'account', 'the', 'entire', 'process', 'can', 'be', 'completed', 'in', 'one', 'day', 'this', 'description', 'is', 'like', 'the', 'plot', 'of', 'a', 'movie', 'but', 'it', 'is', 'really', 'shocking', 'especially', 'the', 'fact', 'that', 'what', 'has', 'been', 'revealed', 'is', 'only', 'the', 'tip', 'of', 'the', 'iceberg', 'given', 'such', 'modern', 'methods', 'of', 'committing', 'crime', 'it', 'has', 'become', 'even', 'more', 'difficult', 'to', 'crack', 'down', 'on', 'crime', 'gelbard', 'pointed', 'out', 'that', 'half', 'the', 'drugs', 'on', 'the', 'us', 'market', 'come', 'from', 'asia', 'with', 'the', 'opening', 'up', 'of', 'the', 'country', 'the', 'economic', 'development', 'and', 'the', 'influx', 'of', 'western', 'ideas', 'various', 'different', 'factors', 'have', 'contributed', 'to', 'the', 'resurrection', 'of', 'drugtaking', 'in', 'china', 'where', 'it', 'had', 'disappeared', 'for', 'decades', 'the', 'drugtrafficking', 'activities', 'through', 'chinas', 'long', 'borders', 'have', 'also', 'become', 'increasingly', 'rampant', 'the', 'crime', 'syndicates', 'in', 'hong', 'kong', 'taiwan', 'and', 'the', 'united', 'states', 'not', 'only', 'smuggle', 'drugs', 'to', 'other', 'countries', 'through', 'the', 'chinese', 'mainland', 'but', 'also', 'smuggle', 'human', 'cargo', 'to', 'the', 'united', 'states', 'in', 'a', 'big', 'way', 'making', 'it', 'more', 'urgent', 'for', 'china', 'and', 'the', 'united', 'states', 'to', 'jointly', 'crack', 'down', 'on', 'such', 'organized', 'crime', 'as', 'a', 'financial', 'and', 'information', 'center', 'hong', 'kong', 'is', 'naturally', 'regarded', 'by', 'the', 'international', 'crime', 'syndicates', 'as', 'a', 'paradise', 'for', 'money', 'laundering', 'and', 'a', 'supreme', 'headquarters', 'for', 'mobilizing', 'and', 'organizing', 'various', 'criminal', 'activities', 'what', 'is', 'needed', 'now', 'is', 'for', 'the', 'worlds', 'lawenforcement', 'authorities', 'to', 'make', 'joint', 'efforts', 'to', 'crack', 'down', 'on', 'the', 'frenzied', 'criminal', 'activities', 'it', 'is', 'gratifying', 'that', 'such', 'joint', 'efforts', 'are', 'being', 'stepped', 'up', 'gelbard', 'said', 'that', 'from', 'his', 'talks', 'with', 'chinese', 'officials', 'he', 'had', 'got', 'the', 'impression', 'that', 'the', 'chinese', 'side', 'approached', 'drugtrafficking', 'and', 'other', 'issues', 'very', 'seriously', 'and', 'conscientiously', 'and', 'that', 'sinous', 'cooperation', 'would', 'be', 'further', 'strengthened', 'his', 'successful', 'visit', 'to', 'china', 'also', 'shows', 'that', 'through', 'their', 'joint', 'efforts', 'china', 'and', 'the', 'united', 'states', 'can', 'more', 'effectively', 'crack', 'down', 'on', 'crimes', 'and', 'safeguard', 'peoples', 'security', 'and', 'social', 'stability']\n"
     ]
    }
   ],
   "source": [
    "doc_idx = doc_id2idx['FBIS3-26415']\n",
    "print(docs[doc_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "770"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(doc_chunk_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate rel(C, d)\n",
    "for i in range(len(test_querues)):\n",
    "    chunks , scores_qc = rel_query_topkdoc_chunks[i]\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
