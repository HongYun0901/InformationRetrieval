{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bm25\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "import pickle\n",
    "import re\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    text = str(text)\n",
    "    text = re.sub(r'[^\\w\\s]','',text)\n",
    "    text = text.lower()\n",
    "    text = text.replace('\\n',' ').replace('\\t' , ' ')\n",
    "    text = text.strip()\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "    \n",
    "doc_df = pd.read_csv('documents.csv')\n",
    "docs_id = doc_df['doc_id'].tolist()\n",
    "docs = doc_df['doc_text'].apply(process_text).tolist()\n",
    "doc_id2idx = {}\n",
    "for i in range(len(docs_id)):\n",
    "    doc_id2idx[docs_id[i]] = i\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n",
      "120\n",
      "120\n",
      "120\n",
      "120\n"
     ]
    }
   ],
   "source": [
    "train_query_df = pd.read_csv('train_queries.csv')\n",
    "train_queries = train_query_df['query_text'].apply(process_text).tolist()\n",
    "train_queries_id = train_query_df['query_id'].astype('str').tolist()\n",
    "train_queries_pos_docs = train_query_df['pos_doc_ids'].tolist()\n",
    "train_queries_top_bm25 = train_query_df['bm25_top1000'].tolist()\n",
    "train_queries_top_bm25_scores = train_query_df['bm25_top1000_scores'].tolist()\n",
    "\n",
    "print(len(train_queries))\n",
    "print(len(train_queries_id))\n",
    "print(len(train_queries_pos_docs))\n",
    "print(len(train_queries_top_bm25))\n",
    "print(len(train_queries_top_bm25_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80\n",
      "80\n",
      "80\n",
      "80\n"
     ]
    }
   ],
   "source": [
    "test_query_df = pd.read_csv('test_queries.csv')\n",
    "test_queries = test_query_df['query_text'].apply(process_text).tolist()\n",
    "test_queries_id = test_query_df['query_id'].astype('str').tolist()\n",
    "test_queries_top_bm25 = test_query_df['bm25_top1000'].tolist()\n",
    "test_queries_top_bm25_scores = test_query_df['bm25_top1000_scores'].tolist()\n",
    "\n",
    "print(len(test_queries))\n",
    "print(len(test_queries_id))\n",
    "print(len(test_queries_top_bm25))\n",
    "print(len(test_queries_top_bm25_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "445\n"
     ]
    }
   ],
   "source": [
    "# query_vocab = set()\n",
    "# for query in train_queries:\n",
    "#     query_vocab.update(query)\n",
    "    \n",
    "# for query in test_queries:\n",
    "#     query_vocab.update(query)\n",
    "# print(len(query_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "442\n",
      "445\n",
      "520.59244\n"
     ]
    }
   ],
   "source": [
    "# vocab = set()\n",
    "# total_len = 0\n",
    "# doc_tfs = []\n",
    "# doc_lens = []\n",
    "# for doc in docs:\n",
    "#     doc_tf = {}\n",
    "#     total_len+=len(doc)\n",
    "#     doc_lens.append(len(doc))\n",
    "#     for word in doc:\n",
    "#         if word in query_vocab:\n",
    "#             vocab.add(word)\n",
    "#         if word in vocab:    \n",
    "#             if word in doc_tf:\n",
    "#                 doc_tf[word] += 1\n",
    "#             else:\n",
    "#                 doc_tf[word] = 1\n",
    "#     doc_tfs.append(doc_tf)\n",
    "# print(len(vocab))\n",
    "# vocab = vocab | query_vocab\n",
    "# print(len(vocab))\n",
    "# avg_doc_len = total_len / len(docs)\n",
    "# print(avg_doc_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # hyper params\n",
    "# K1 = 0.8\n",
    "# b = 0.7\n",
    "# K3 = 1000\n",
    "# delta  = 0.2\n",
    "\n",
    "# # Discriminative power\n",
    "# doc_idfs = {}\n",
    "# for word in vocab:\n",
    "#     # avoid division by zero\n",
    "#     ni = 0\n",
    "#     for doc_tf in doc_tfs:\n",
    "#         if word in doc_tf:\n",
    "#             ni += 1\n",
    "#     idf = math.log((len(docs_id) - ni + 0.5) / (ni + 0.5))\n",
    "#     doc_idfs[word] = idf\n",
    "\n",
    "\n",
    "# # compute tfidf\n",
    "# doc_tfidf = []\n",
    "# idx = 0\n",
    "# for doc_tf in doc_tfs:\n",
    "#     tfidf = []\n",
    "#     doc_len_normalize = doc_lens[idx] / avg_doc_len\n",
    "#     idx += 1\n",
    "#     for word in query_vocab:\n",
    "#         # calculate tf\n",
    "#         tf = 0\n",
    "#         if word in doc_tf:\n",
    "#             tf = doc_tf[word]\n",
    "#         if tf > 0:\n",
    "#             tf = tf / (1 - b + b * doc_len_normalize)\n",
    "#             tf = ((K1 + 1) * (tf + delta)) / (K1 + tf + delta)\n",
    "#         idf = doc_idfs[word]\n",
    "#         tfidf.append(tf * idf * (idf))\n",
    "#     doc_tfidf.append(tfidf)\n",
    "# doc_tfidf = np.array(doc_tfidf)\n",
    "\n",
    "# print(doc_tfidf.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def map_score(x,ans):\n",
    "#     score = 0.0\n",
    "#     results = x\n",
    "#     relates = ans\n",
    "#     idx = 0\n",
    "#     count = 0\n",
    "#     for result in results:\n",
    "#         if result in relates:\n",
    "#             count += 1\n",
    "#             score += count / (idx + 1)\n",
    "#         idx += 1\n",
    "#     score /= len(relates)\n",
    "#     return score\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BM25 = []\n",
    "# query_idx = 0\n",
    "# _map = 0.0\n",
    "# for query_id, query in zip(train_queries_id, train_queries):\n",
    "#     query_tf = [0] * len(vocab)\n",
    "#     for q in query:\n",
    "#         idx = 0\n",
    "#         for word in vocab:\n",
    "#             if word == q:\n",
    "#                 query_tf[idx] += 1\n",
    "#                 if query_tf[idx] > 1:\n",
    "#                     print('repeat word in query')\n",
    "#                 break\n",
    "#             idx += 1\n",
    "#     for i in range(len(query_vocab)):\n",
    "#         query_tf[i] = ((K3+1) * query_tf[i]) / (K3+query_tf[i])\n",
    "#     query_tf = np.array(query_tf)\n",
    "#     relevance = np.matmul(doc_tfidf, query_tf)\n",
    "#     query_bm25_doc_idx = np.argsort(relevance)[-1000:]\n",
    "#     BM25.append(query_bm25_doc_idx)\n",
    "# #     label\n",
    "#     query_bm25_ans = train_queries_top_bm25[query_idx].split()\n",
    "#     query_bm25 = []\n",
    "#     for doc_idx in query_bm25_doc_idx:\n",
    "#         query_bm25.append(docs_id[doc_idx])\n",
    "\n",
    "#     _ap = map_score(query_bm25,query_bm25_ans)\n",
    "#     _map += _ap\n",
    "#     query_idx += 1\n",
    "\n",
    "# print(_map / len(train_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use my self bm25\n",
    "\n",
    "# x_qd = []\n",
    "# train_y = []\n",
    "\n",
    "# for query_idx,query in enumerate(train_queries):\n",
    "#     query = ' '.join(query)\n",
    "# #     BM25找回來的\n",
    "#     bm25_topk = BM25[query_idx]\n",
    "# #     query的正確答案\n",
    "#     query_pos_doc_idx = []\n",
    "#     query_pos_doc_id = train_queries_pos_docs[query_idx].split()\n",
    "#     count = 0\n",
    "#     for doc_id in query_pos_doc_id:\n",
    "#         doc_idx = doc_id2idx[doc_id]\n",
    "#         if doc_idx in bm25_topk:\n",
    "#             count += 1\n",
    "#         else:\n",
    "#             document = ' '.join(docs[doc_idx])\n",
    "#             x_qd.append((query,document))\n",
    "#             train_y.append(1)\n",
    "            \n",
    "#         query_pos_doc_idx.append(doc_idx)\n",
    "#     count = 0\n",
    "#     for doc_idx in bm25_topk:\n",
    "#         document = ' '.join(docs[doc_idx])\n",
    "#         if doc_idx in query_pos_doc_idx:\n",
    "#             x_qd.append((query,document))\n",
    "#             train_y.append(1)\n",
    "#             count += 1\n",
    "#         else:\n",
    "# #             if np.random.random()<0.1:\n",
    "#             x_qd.append((query,document))\n",
    "#             train_y.append(0)\n",
    "\n",
    "# # x_d = np.array(x_d)\n",
    "# # x_q = np.array(x_q)\n",
    "# # y = np.array(y)\n",
    "# print(len(x_qd))\n",
    "# print(len(train_y))\n",
    "\n",
    "# print(sum(train_y) / len(train_y))\n",
    "# import torch\n",
    "# train_y = torch.tensor(train_y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "120000\n",
      "0.047825\n"
     ]
    }
   ],
   "source": [
    "x_qd = []\n",
    "train_y = []\n",
    "\n",
    "for query_idx,query in enumerate(train_queries):\n",
    "    query = ' '.join(query)\n",
    "#     BM25找回來的\n",
    "    bm25_topk = train_queries_top_bm25[query_idx].split()\n",
    "#     query的正確答案\n",
    "    query_pos_doc_idx = []\n",
    "    query_pos_doc_id = train_queries_pos_docs[query_idx].split()\n",
    "    for doc_id in query_pos_doc_id:\n",
    "        doc_idx = doc_id2idx[doc_id]  \n",
    "        query_pos_doc_idx.append(doc_idx)\n",
    "        \n",
    "    for doc_id in bm25_topk:\n",
    "        doc_idx = doc_id2idx[doc_id]\n",
    "        document = ' '.join(docs[doc_idx])\n",
    "        if doc_idx in query_pos_doc_idx:\n",
    "            x_qd.append((query,document))\n",
    "            train_y.append(1)\n",
    "        else:\n",
    "            x_qd.append((query,document))\n",
    "            train_y.append(0)\n",
    "        \n",
    "\n",
    "\n",
    "print(len(x_qd))\n",
    "print(len(train_y))\n",
    "\n",
    "print(sum(train_y) / len(train_y))\n",
    "import torch\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "906851ea37a74c3fb32f1e5a78dcb056",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=231508.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from transformers import RobertaTokenizer, RobertaForQuestionAnswering , RobertaForSequenceClassification\n",
    "from transformers import BertTokenizer, RobertaForQuestionAnswering , RobertaForSequenceClassification\n",
    "\n",
    "# pretrained_model = 'roberta-large'\n",
    "pretrained_model = 'bert-base-uncased'\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(pretrained_model)\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 120000/120000 [38:24<00:00, 52.06it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120000\n",
      "120000\n",
      "120000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "input_dict = {}\n",
    "input_ids = []\n",
    "attention_mask = []\n",
    "token_type_ids = []\n",
    "\n",
    "for i in tqdm(range(len(x_qd))):\n",
    "    x = x_qd[i]\n",
    "    tmp_dict =  tokenizer.encode_plus(x[0],\n",
    "                          x[1],\n",
    "                          max_length=512,\n",
    "                          return_tensors='pt',\n",
    "                          return_token_type_ids = True,\n",
    "                          pad_to_max_length=True,\n",
    "                          truncation=True)\n",
    "    input_ids.append(tmp_dict['input_ids'][0])\n",
    "    attention_mask.append(tmp_dict['attention_mask'][0])\n",
    "    token_type_ids.append(tmp_dict['token_type_ids'][0])\n",
    "    \n",
    "#     print(input_ids[0])\n",
    "#     print(attention_mask[0])\n",
    "#     print(token_type_ids[0])\n",
    "#     input()\n",
    "\n",
    "\n",
    "print(len(input_ids))\n",
    "print(len(attention_mask))\n",
    "print(len(token_type_ids))\n",
    "\n",
    "\n",
    "# with open('input_dict{}'.format(len(x_qd)), 'wb') as handle:\n",
    "#     pickle.dump(input_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# with open('train_y{}'.format(len(x_qd)), 'wb') as handle:\n",
    "#     pickle.dump(train_y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('{}_input_ids{}'.format(pretrained_model,len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(input_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('{}_attention_mask{}'.format(pretrained_model,len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(attention_mask, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('{}_token_type_ids{}'.format(pretrained_model,len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(token_type_ids, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "with open('train_y{}'.format(len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(train_y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/124168 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "100%|██████████| 124168/124168 [3:41:08<00:00,  9.36it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([124168, 512])\n",
      "torch.Size([124168, 512])\n",
      "torch.Size([124168, 512])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# input_dict = {}\n",
    "\n",
    "# count = 0 \n",
    "# for i in tqdm(range(len(x_qd))):\n",
    "#     x = x_qd[i]\n",
    "#     tmp_dict =  tokenizer(x[0],\n",
    "#                           x[1],\n",
    "#                           max_length=512,\n",
    "#                           return_tensors='pt',\n",
    "#                           return_token_type_ids = True,\n",
    "#                           pad_to_max_length=True,\n",
    "#                           truncation=True)\n",
    "# #     print(tmp_dict['input_ids'].size())\n",
    "# #     print(tmp_dict['attention_mask'].size())\n",
    "# #     print(tmp_dict['token_type_ids'].size())\n",
    "\n",
    "#     if i == 0 :\n",
    "#         input_dict = tmp_dict\n",
    "#     else:\n",
    "        \n",
    "#         input_dict['input_ids'] = torch.cat((input_dict['input_ids'],tmp_dict['input_ids']),dim=0)\n",
    "#         input_dict['attention_mask'] = torch.cat((input_dict['attention_mask'],tmp_dict['attention_mask']),dim=0)\n",
    "#         input_dict['token_type_ids'] = torch.cat((input_dict['token_type_ids'],tmp_dict['token_type_ids']),dim=0)\n",
    "        \n",
    "    \n",
    "\n",
    "# print(input_dict['input_ids'].size())\n",
    "# print(input_dict['attention_mask'].size())\n",
    "# print(input_dict['token_type_ids'].size())\n",
    "\n",
    "\n",
    "# # with open('input_dict{}'.format(len(x_qd)), 'wb') as handle:\n",
    "# #     pickle.dump(input_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "# # with open('train_y{}'.format(len(x_qd)), 'wb') as handle:\n",
    "# #     pickle.dump(train_y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_model = 'roberta-large'\n",
    "with open('{}_input_dict{}'.format(pretrained_model,len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(input_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "with open('{}_train_y{}'.format(pretrained_model,len(x_qd)), 'wb') as handle:\n",
    "    pickle.dump(train_y, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(input_dict['input_ids'],'input_ids{}'.format(len(x_qd)))\n",
    "torch.save(input_dict['attention_mask'],'attention_mask{}'.format(len(x_qd)))\n",
    "torch.save(input_dict['token_type_ids'],'token_type_ids{}'.format(len(x_qd)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('input_dict', 'rb') as handle:\n",
    "    input_dict = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, input_dict, y):\n",
    "        self.input_ids = input_dict['input_ids']\n",
    "        self.token_type_ids = input_dict['token_type_ids']\n",
    "        self.attention_mask = input_dict['attention_mask']\n",
    "        self.y = y\n",
    "        \n",
    "    def __getitem__(self,idx):\n",
    "        inputid = self.input_ids[idx]\n",
    "        tokentype = self.token_type_ids[idx]\n",
    "        attentionmask = self.attention_mask[idx]\n",
    "        y = self.y[idx]\n",
    "        return inputid , tokentype , attentionmask, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "    \n",
    "# def get_predictions(model, loader, BATCH_SIZE):\n",
    "#     result = []\n",
    "#     total_count = 0 # 第n筆data\n",
    "#     model.eval()\n",
    "#     with torch.no_grad():\n",
    "#         for data in loader:\n",
    "#             tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "#             tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "#             outputs = model(input_ids=tokens_tensors, \n",
    "#                       token_type_ids=segments_tensors, \n",
    "#                       attention_mask=masks_tensors)\n",
    "#             pred = outputs[0].argmax(1)\n",
    "\n",
    "#             result.append(r)\n",
    "#             total_count += 1\n",
    "\n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_qd = []\n",
    "train_y = []\n",
    "\n",
    "for query_idx,query in enumerate(train_queries):\n",
    "    query = ' '.join(query)\n",
    "#     BM25找回來的\n",
    "    bm25_topk = BM25[query_idx]\n",
    "#     query的正確答案\n",
    "    query_pos_doc_idx = []\n",
    "    query_pos_doc_id = train_queries_pos_docs[query_idx].split()\n",
    "    count = 0\n",
    "    for doc_id in query_pos_doc_id:\n",
    "        doc_idx = doc_id2idx[doc_id]\n",
    "        if doc_idx in bm25_topk:\n",
    "            count += 1\n",
    "        else:\n",
    "            document = ' '.join(docs[doc_idx])\n",
    "            x_qd.append((query,document))\n",
    "            train_y.append(1)\n",
    "        query_pos_doc_idx.append(doc_idx)\n",
    "#     print('label num : {} , BM25 find num {}'.format(len(query_pos_doc_id),count))\n",
    "    count = 0\n",
    "    for doc_idx in bm25_topk:\n",
    "        document = ' '.join(docs[doc_idx])\n",
    "        if doc_idx in query_pos_doc_idx:\n",
    "            x_qd.append((query,document))\n",
    "            train_y.append(1)\n",
    "            count += 1\n",
    "        else:\n",
    "#             if np.random.random()<0.1:\n",
    "            x_qd.append((query,document))\n",
    "            train_y.append(0)\n",
    "            \n",
    "#     print('in {} data , {} data label is true, {} data label is false '.format(len(bm25_topk),count,len(bm25_topk)-count))\n",
    "\n",
    "\n",
    "# x_d = np.array(x_d)\n",
    "# x_q = np.array(x_q)\n",
    "# y = np.array(y)\n",
    "print(len(x_qd))\n",
    "print(len(train_y))\n",
    "\n",
    "print(sum(train_y) / len(train_y))\n",
    "import torch\n",
    "train_y = torch.tensor(train_y)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " epoch:0/4 acc rate : 0.4350594227504245 data:2356/52188"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20861 is out of bounds for dimension 0 with size 20847",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-120bb98bfca3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mcount\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mcorrect_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainLoader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mtokens_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegments_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    433\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    473\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    476\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-591e3e8a057f>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mtokentype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mattentionmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minputid\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mtokentype\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mattentionmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20861 is out of bounds for dimension 0 with size 20847"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "weight = torch.FloatTensor([1, 10]).cuda()\n",
    "binary_loss_fct = torch.nn.CrossEntropyLoss(weight=weight)\n",
    "BATCH_SIZE = 4\n",
    "trainSet = TrainDataset(input_dict, y)\n",
    "trainLoader = DataLoader(trainSet, batch_size=BATCH_SIZE,shuffle=True)\n",
    "model = RobertaForSequenceClassification.from_pretrained(pretrained_model)\n",
    "\n",
    "device = 'cuda:0'\n",
    "model = model.to(device)\n",
    "model.train() \n",
    "\n",
    "EPOCHS = 4\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    correct_count = 0\n",
    "    for data in trainLoader:\n",
    "        tokens_tensors, segments_tensors, masks_tensors, labels = [t.to(device) for t in data]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                      token_type_ids=segments_tensors, \n",
    "                      attention_mask=masks_tensors)\n",
    "        \n",
    "        pred = outputs[0].argmax(1)\n",
    "        count += labels.size()[0]\n",
    "        correct_count += (labels==pred).sum().cpu().item()\n",
    "        \n",
    "        loss = binary_loss_fct(outputs[0],labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        print(f'\\r epoch:{epoch}/{EPOCHS} acc rate : {correct_count/count} data:{count}/{len(trainLoader)}',end='')\n",
    "        \n",
    "    CHECKPOINT_NAME = './roberta_large_epoch{}_train_size{}'.format(epoch,20872) \n",
    "    torch.save(model.state_dict(), CHECKPOINT_NAME)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
